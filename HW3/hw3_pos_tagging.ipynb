{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "hw3-pos-tagging.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpCKO19dhHPq",
        "colab_type": "text"
      },
      "source": [
        "# HW 3 - Neural POS Tagger\n",
        "\n",
        "In this exercise, you are going to build a set of deep learning models on part-of-speech (POS) tagging using Tensorflow and Keras. Tensorflow is a deep learning framwork developed by Google, and Keras is a frontend library built on top of Tensorflow (or Theano, CNTK) to provide an easier way to use standard layers and networks.\n",
        "\n",
        "To complete this exercise, you will need to build deep learning models for POS tagging in Thai using NECTEC's ORCHID corpus. You will build one model for each of the following type:\n",
        "\n",
        "- Neural POS Tagging with Word Embedding using Fixed / non-Fixed Pretrained weights\n",
        "- Neural POS Tagging with Viterbi / Marginal CRF\n",
        "\n",
        "Pretrained word embeddding are already given for you to use (albeit, a very bad one).\n",
        "\n",
        "We also provide the code for data cleaning, preprocessing and some starter code for keras in this notebook but feel free to modify those parts to suit your needs. You can also complete this exercise using only Tensorflow (without using Keras). Feel free to use additional libraries (e.g. scikit-learn) as long as you have a model for each type mentioned above.\n",
        "\n",
        "### Don't forget to change hardware accelrator to GPU in runtime on Google Colab ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx0YbM_8hHPt",
        "colab_type": "text"
      },
      "source": [
        "## 1. Setup and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF3lPNNkhHPu",
        "colab_type": "text"
      },
      "source": [
        "We use POS data from [ORCHID corpus](https://www.nectec.or.th/corpus/index.php?league=pm), which is a POS corpus for Thai language.\n",
        "A method used to read the corpus into a list of sentences with (word, POS) pairs have been implemented already. The example usage has shown below.\n",
        "We also create a word vector for unknown word by random."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS6O5yT6feRd",
        "colab_type": "code",
        "outputId": "3c1c37fc-2de4-4fcc-b772-c3f0e9d42768",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "source": [
        "!pip install keras==2.2.4"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==2.2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
            "\r\u001b[K     |█                               | 10kB 19.2MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |████▏                           | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 296kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 307kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.1.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.17.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (2.8.0)\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.2.5\n",
            "    Uninstalling Keras-2.2.5:\n",
            "      Successfully uninstalled Keras-2.2.5\n",
            "Successfully installed keras-2.2.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58LvFz30zumq",
        "colab_type": "code",
        "outputId": "b0df25fc-0832-4f02-fdd8-8f7521e773eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        }
      },
      "source": [
        "!wget https://www.dropbox.com/s/tuvrbsby4a5axe0/resources.zip\n",
        "!unzip resources.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-14 00:10:34--  https://www.dropbox.com/s/tuvrbsby4a5axe0/resources.zip\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.82.1, 2620:100:6032:1::a27d:5201\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.82.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/tuvrbsby4a5axe0/resources.zip [following]\n",
            "--2020-01-14 00:10:34--  https://www.dropbox.com/s/raw/tuvrbsby4a5axe0/resources.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc0723499c1370db56c0cc76e7f1.dl.dropboxusercontent.com/cd/0/inline/AwG9igZ0uR5wN8rEPti8Kq_du_KbnD6o_oIHQ5NoWX4qZ4GOGYL6j1BNQF9iZ-ZLGoiVhdeAG-wrCUCIvLqF2O19vCc_YYgvDw6_L6tO7WSPUw/file# [following]\n",
            "--2020-01-14 00:10:35--  https://uc0723499c1370db56c0cc76e7f1.dl.dropboxusercontent.com/cd/0/inline/AwG9igZ0uR5wN8rEPti8Kq_du_KbnD6o_oIHQ5NoWX4qZ4GOGYL6j1BNQF9iZ-ZLGoiVhdeAG-wrCUCIvLqF2O19vCc_YYgvDw6_L6tO7WSPUw/file\n",
            "Resolving uc0723499c1370db56c0cc76e7f1.dl.dropboxusercontent.com (uc0723499c1370db56c0cc76e7f1.dl.dropboxusercontent.com)... 162.125.82.6, 2620:100:6032:6::a27d:5206\n",
            "Connecting to uc0723499c1370db56c0cc76e7f1.dl.dropboxusercontent.com (uc0723499c1370db56c0cc76e7f1.dl.dropboxusercontent.com)|162.125.82.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: /cd/0/inline2/AwEllGQeydBCs5_lI3RIQF8-2bMDOpeawIBJ0CIDovBCk9n2YDD12KwS4jmXjZTTH9fu5zLCpFAU3gPAi773_AsxJT7mCxALXwHLcWgnhy2XMigznsvNb81XtUtcqRUQV2J3LkWtjuJ5WxqtctuxW9DLe_kM41ILhTOSj8dmla3cquJAapjfzkb6SdAyKtlax7oT8wpzZbQANweOAFkSYa0eNlmxld7N1Wk25tmqkiDES0zpCRCGzNdJ9B7YXyOj4Ps1gU8A1hozexB214qEfyKjTFKY4Y8Sag4AKxwl_qQXPlufP3SZrNjXVeBUkn3ldZUKnq5D5-u_H8jTqYEIYYIv/file [following]\n",
            "--2020-01-14 00:10:36--  https://uc0723499c1370db56c0cc76e7f1.dl.dropboxusercontent.com/cd/0/inline2/AwEllGQeydBCs5_lI3RIQF8-2bMDOpeawIBJ0CIDovBCk9n2YDD12KwS4jmXjZTTH9fu5zLCpFAU3gPAi773_AsxJT7mCxALXwHLcWgnhy2XMigznsvNb81XtUtcqRUQV2J3LkWtjuJ5WxqtctuxW9DLe_kM41ILhTOSj8dmla3cquJAapjfzkb6SdAyKtlax7oT8wpzZbQANweOAFkSYa0eNlmxld7N1Wk25tmqkiDES0zpCRCGzNdJ9B7YXyOj4Ps1gU8A1hozexB214qEfyKjTFKY4Y8Sag4AKxwl_qQXPlufP3SZrNjXVeBUkn3ldZUKnq5D5-u_H8jTqYEIYYIv/file\n",
            "Reusing existing connection to uc0723499c1370db56c0cc76e7f1.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 151484222 (144M) [application/zip]\n",
            "Saving to: ‘resources.zip’\n",
            "\n",
            "resources.zip       100%[===================>] 144.47M  44.6MB/s    in 3.2s    \n",
            "\n",
            "2020-01-14 00:10:40 (44.6 MB/s) - ‘resources.zip’ saved [151484222/151484222]\n",
            "\n",
            "Archive:  resources.zip\n",
            "  inflating: basic_ff_embedding.pt   \n",
            "   creating: data/\n",
            " extracting: data/__init__.py        \n",
            "  inflating: data/__init__.pyc       \n",
            "   creating: data/__pycache__/\n",
            "  inflating: data/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: data/__pycache__/orchid_corpus.cpython-36.pyc  \n",
            "  inflating: data/orchid97.txt       \n",
            "  inflating: data/orchid_corpus.py   \n",
            "  inflating: data/orchid_corpus.pyc  \n",
            "  inflating: data/orchid_test.txt    \n",
            "  inflating: data/orchid_train.txt   \n",
            "   creating: embeddings/\n",
            " extracting: embeddings/__init__.py  \n",
            "   creating: embeddings/__pycache__/\n",
            "  inflating: embeddings/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: embeddings/__pycache__/emb_reader.cpython-36.pyc  \n",
            "  inflating: embeddings/emb_reader.py  \n",
            "  inflating: embeddings/polyglot-th.pkl  \n",
            "   creating: model/\n",
            "  inflating: model/_DS_Store         \n",
            "  inflating: model/crf_basic.model   \n",
            "  inflating: model/crf_neural.model  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2a9b92hYTg2",
        "colab_type": "code",
        "outputId": "dec63140-3287-484b-d2d8-ea9ec56b449a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "!pip install python-crfsuite\n",
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python-crfsuite\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2f/86/cfcd71edca9d25d3d331209a20f6314b6f3f134c29478f90559cee9ce091/python_crfsuite-0.9.6-cp36-cp36m-manylinux1_x86_64.whl (754kB)\n",
            "\u001b[K     |████████████████████████████████| 757kB 2.7MB/s \n",
            "\u001b[?25hInstalling collected packages: python-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.6\n",
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-q8_1vo_n\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-q8_1vo_n\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-contrib==2.0.8) (2.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.17.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp36-none-any.whl size=101065 sha256=303f3a1417d3099115d466a5abf528fb4bba3eb20ed7faea534ee9e088f88897\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-x68wxpqg/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n",
            "Successfully built keras-contrib\n",
            "Installing collected packages: keras-contrib\n",
            "Successfully installed keras-contrib-2.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_c7upY0fYsdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJpCbSvJhHPv",
        "colab_type": "code",
        "outputId": "a78531a8-089b-4746-d580-58dad423c1ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from data.orchid_corpus import get_sentences\n",
        "import numpy as np\n",
        "import numpy.random\n",
        "import keras.preprocessing\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "U4SZz56ThHP0",
        "colab_type": "code",
        "outputId": "7f59a509-d073-4aa6-d8ba-78a146b460c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "unk_emb =np.random.randn(32)\n",
        "train_data = get_sentences('train')\n",
        "test_data = get_sentences('test')\n",
        "print(train_data[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('การ', 'FIXN'), ('ประชุม', 'VACT'), ('ทาง', 'NCMN'), ('วิชาการ', 'NCMN'), ('<space>', 'PUNC'), ('ครั้ง', 'CFQC'), ('ที่ 1', 'DONM')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awxn_GRIhHP3",
        "colab_type": "text"
      },
      "source": [
        "Next, we load pretrained weight embedding using pickle. The pretrained weight is a dictionary which map a word to its embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GS3lTZshHP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "fp = open('basic_ff_embedding.pt', 'rb')\n",
        "embeddings = pickle.load(fp)\n",
        "fp.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CqTNKsChHP7",
        "colab_type": "text"
      },
      "source": [
        "The given code below generates an indexed dataset(each word is represented by a number) for training and testing data. The index 0 is reserved for padding to help with variable length sequence. (Additionally, You can read more about padding here [https://machinelearningmastery.com/data-preparation-variable-length-input-sequences-sequence-prediction/])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPGUNEZyhHP8",
        "colab_type": "text"
      },
      "source": [
        "## 2. Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fMWn8qehHP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_idx ={}\n",
        "idx_to_word ={}\n",
        "label_to_idx = {}\n",
        "for sentence in train_data:\n",
        "    for word,pos in sentence:\n",
        "        if word not in word_to_idx:\n",
        "            word_to_idx[word] = len(word_to_idx)+1\n",
        "            idx_to_word[word_to_idx[word]] = word\n",
        "        if pos not in label_to_idx:\n",
        "            label_to_idx[pos] = len(label_to_idx)+1\n",
        "word_to_idx['UNK'] = len(word_to_idx)\n",
        "\n",
        "n_classes = len(label_to_idx.keys())+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgvZ8v_2hHP_",
        "colab_type": "text"
      },
      "source": [
        "This section is tweaked a little from the demo, word2features will return word index instead of features, and sent2labels will return a sequence of word indices in the sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktf2KkJghHQA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word2features(sent, i, emb):\n",
        "    word = sent[i][0]\n",
        "    if word in word_to_idx :\n",
        "        return word_to_idx[word]\n",
        "    else :\n",
        "        return word_to_idx['UNK']\n",
        "\n",
        "def sent2features(sent, emb_dict):\n",
        "    return np.asarray([word2features(sent, i, emb_dict) for i in range(len(sent))])\n",
        "\n",
        "def sent2labels(sent):\n",
        "    return numpy.asarray([label_to_idx[label] for (word, label) in sent],dtype='int32')\n",
        "\n",
        "def sent2tokens(sent):\n",
        "    return [word for (word, label) in sent]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgBw3I9ShHQD",
        "colab_type": "code",
        "outputId": "2be4e126-4a2c-42c0-d2b2-d720d9fee840",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sent2features(train_data[100], embeddings)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 29, 327,   5, 328])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O7oClK-hHQG",
        "colab_type": "text"
      },
      "source": [
        "Next we create train and test dataset, then we use keras to post-pad the sequence to max sequence with 0. Our labels are changed to a one-hot vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tJxtPtohHQH",
        "colab_type": "code",
        "outputId": "a9ee056e-5c2e-49a5-ad30-5b32cf6c7ff3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "x_train = np.asarray([sent2features(sent, embeddings) for sent in train_data])\n",
        "y_train = [sent2labels(sent) for sent in train_data]\n",
        "x_test = [sent2features(sent, embeddings) for sent in test_data]\n",
        "y_test = [sent2labels(sent) for sent in test_data]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 314 ms, sys: 2.31 ms, total: 316 ms\n",
            "Wall time: 317 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DG1gvJ4mhHQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train=keras.preprocessing.sequence.pad_sequences(x_train, maxlen=None, dtype='int32', padding='post', truncating='pre', value=0.)\n",
        "y_train=keras.preprocessing.sequence.pad_sequences(y_train, maxlen=None, dtype='int32', padding='post', truncating='pre', value=0.)\n",
        "x_test=keras.preprocessing.sequence.pad_sequences(x_test, maxlen=102, dtype='int32', padding='post', truncating='pre', value=0.)\n",
        "y_temp =[]\n",
        "for i in range(len(y_train)):\n",
        "    y_temp.append(np.eye(n_classes)[y_train[i]][np.newaxis,:])\n",
        "y_train = np.asarray(y_temp).reshape(-1,102,n_classes)\n",
        "del(y_temp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZU9x6VdehHQM",
        "colab_type": "code",
        "outputId": "58ba9b7f-6318-4d84-f8d8-80a79eededcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "print(x_train[100],x_train.shape)\n",
        "print(y_train[100][3],y_train.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 29 327   5 328   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0] (18500, 102)\n",
            "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] (18500, 102, 48)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx_c3-LwhHQP",
        "colab_type": "text"
      },
      "source": [
        "## 3. Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvCW24orhHQP",
        "colab_type": "text"
      },
      "source": [
        "Our output from keras is a distribution of problabilities on all possible label. outputToLabel will return an indices of maximum problability from output sequence.\n",
        "\n",
        "evaluation_report is the same as in the demo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iqgp2gd4hHQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def outputToLabel(yt,seq_len):\n",
        "    out = []\n",
        "    for i in range(0,len(yt)):\n",
        "        if(i==seq_len):\n",
        "            break\n",
        "        out.append(np.argmax(yt[i]))\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzIL_rsAhHQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "def evaluation_report(y_true, y_pred):\n",
        "    # retrieve all tags in y_true\n",
        "    tag_set = set()\n",
        "    for sent in y_true:\n",
        "        for tag in sent:\n",
        "            tag_set.add(tag)\n",
        "    for sent in y_pred:\n",
        "        for tag in sent:\n",
        "            tag_set.add(tag)\n",
        "    tag_list = sorted(list(tag_set))\n",
        "    \n",
        "    # count correct points\n",
        "    tag_info = dict()\n",
        "    for tag in tag_list:\n",
        "        tag_info[tag] = {'correct_tagged': 0, 'y_true': 0, 'y_pred': 0}\n",
        "\n",
        "    all_correct = 0\n",
        "    all_count = sum([len(sent) for sent in y_true])\n",
        "    for sent_true, sent_pred in zip(y_true, y_pred):\n",
        "        for tag_true, tag_pred in zip(sent_true, sent_pred):\n",
        "            if tag_true == tag_pred:\n",
        "                tag_info[tag_true]['correct_tagged'] += 1\n",
        "                all_correct += 1\n",
        "            tag_info[tag_true]['y_true'] += 1\n",
        "            tag_info[tag_pred]['y_pred'] += 1\n",
        "    accuracy = (all_correct / all_count) * 100\n",
        "            \n",
        "    # summarize and make evaluation result\n",
        "    eval_list = list()\n",
        "    for tag in tag_list:\n",
        "        eval_result = dict()\n",
        "        eval_result['tag'] = tag\n",
        "        eval_result['correct_count'] = tag_info[tag]['correct_tagged']\n",
        "        precision = (tag_info[tag]['correct_tagged']/tag_info[tag]['y_pred'])*100 if tag_info[tag]['y_pred'] else '-'\n",
        "        recall = (tag_info[tag]['correct_tagged']/tag_info[tag]['y_true'])*100 if (tag_info[tag]['y_true'] > 0) else 0\n",
        "        eval_result['precision'] = precision\n",
        "        eval_result['recall'] = recall\n",
        "        eval_result['f_score'] = (2*precision*recall)/(precision+recall) if (type(precision) is float and recall > 0) else '-'\n",
        "        \n",
        "        eval_list.append(eval_result)\n",
        "\n",
        "    eval_list.append({'tag': 'accuracy=%.2f' % accuracy, 'correct_count': '', 'precision': '', 'recall': '', 'f_score': ''})\n",
        "    \n",
        "    df = pd.DataFrame.from_dict(eval_list)\n",
        "    df = df[['tag', 'precision', 'recall', 'f_score', 'correct_count']]\n",
        "    display(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YG-RiHdhHQV",
        "colab_type": "text"
      },
      "source": [
        "## 4. Train a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HrkAiMFhHQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Embedding, Reshape, Activation, Input, Dense,GRU,Reshape,TimeDistributed,Bidirectional,Dropout,Masking\n",
        "from keras_contrib.layers import CRF\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U75Ivn2vhHQZ",
        "colab_type": "text"
      },
      "source": [
        "The model is this section is separated to two groups\n",
        "\n",
        "- Neural POS Tagger (4.1)\n",
        "- Neural CRF POS Tagger (4.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLwL3B7rhHQZ",
        "colab_type": "text"
      },
      "source": [
        "## 4.1.1 Neural POS Tagger  (Example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVoB-1XVhHQa",
        "colab_type": "text"
      },
      "source": [
        "We create a simple Neural POS Tagger as an example for you. This model dosen't use any pretrained word embbeding so it need to use Embedding layer to train the word embedding from scratch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxBvv9qfhHQb",
        "colab_type": "code",
        "outputId": "dbf11458-8291-4309-d71e-7528998011a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(len(word_to_idx),32,input_length=102,mask_zero=True))\n",
        "model.add(Bidirectional(GRU(32, return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(TimeDistributed(Dense(n_classes,activation='softmax')))\n",
        "model.summary()\n",
        "adam  = Adam(lr=0.001)\n",
        "model.compile(optimizer=adam,  loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3239: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 102, 32)           480608    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 102, 64)           12480     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 102, 64)           0         \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 102, 48)           3120      \n",
            "=================================================================\n",
            "Total params: 496,208\n",
            "Trainable params: 496,208\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wo9Da8MThHQf",
        "colab_type": "code",
        "outputId": "aa7d6fa2-8779-4efe-acdc-ad9f2f149c31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "source": [
        "%%time\n",
        "model.fit(x_train,y_train,batch_size=64,epochs=10,verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "18500/18500 [==============================] - 45s 2ms/step - loss: 1.9034 - categorical_accuracy: 0.5451\n",
            "Epoch 2/10\n",
            "18500/18500 [==============================] - 43s 2ms/step - loss: 0.4280 - categorical_accuracy: 0.9022\n",
            "Epoch 3/10\n",
            "18500/18500 [==============================] - 42s 2ms/step - loss: 0.2554 - categorical_accuracy: 0.9351\n",
            "Epoch 4/10\n",
            "18500/18500 [==============================] - 42s 2ms/step - loss: 0.1998 - categorical_accuracy: 0.9459\n",
            "Epoch 5/10\n",
            "18500/18500 [==============================] - 42s 2ms/step - loss: 0.1722 - categorical_accuracy: 0.9521\n",
            "Epoch 6/10\n",
            "18500/18500 [==============================] - 42s 2ms/step - loss: 0.1564 - categorical_accuracy: 0.9551\n",
            "Epoch 7/10\n",
            "18500/18500 [==============================] - 42s 2ms/step - loss: 0.1446 - categorical_accuracy: 0.9578\n",
            "Epoch 8/10\n",
            "18500/18500 [==============================] - 42s 2ms/step - loss: 0.1362 - categorical_accuracy: 0.9600\n",
            "Epoch 9/10\n",
            "18500/18500 [==============================] - 41s 2ms/step - loss: 0.1290 - categorical_accuracy: 0.9617\n",
            "Epoch 10/10\n",
            "18500/18500 [==============================] - 42s 2ms/step - loss: 0.1229 - categorical_accuracy: 0.9630\n",
            "CPU times: user 12min 57s, sys: 15.4 s, total: 13min 13s\n",
            "Wall time: 7min 3s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f422db14ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yusa214hhHQh",
        "colab_type": "code",
        "outputId": "7811fd90-67b1-4136-fd99-ffd95fb6b06c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "#model.save_weights('/data/my_pos_no_crf.h5')\n",
        "#model.load_weights('/data/my_pos_no_crf.h5')\n",
        "y_pred=model.predict(x_test)\n",
        "ypred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))]\n",
        "evaluation_report(y_test, ypred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tag</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f_score</th>\n",
              "      <th>correct_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>99.8092</td>\n",
              "      <td>99.3758</td>\n",
              "      <td>99.5921</td>\n",
              "      <td>3662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>94.8054</td>\n",
              "      <td>94.4835</td>\n",
              "      <td>94.6442</td>\n",
              "      <td>7793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>91.0111</td>\n",
              "      <td>96.5184</td>\n",
              "      <td>93.6839</td>\n",
              "      <td>16301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>99.9766</td>\n",
              "      <td>99.3654</td>\n",
              "      <td>99.6701</td>\n",
              "      <td>12840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>91.6667</td>\n",
              "      <td>98.5075</td>\n",
              "      <td>94.964</td>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>99.7817</td>\n",
              "      <td>87.5479</td>\n",
              "      <td>93.2653</td>\n",
              "      <td>457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>97.6374</td>\n",
              "      <td>97.4026</td>\n",
              "      <td>97.5199</td>\n",
              "      <td>2025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>67.7812</td>\n",
              "      <td>53.7349</td>\n",
              "      <td>59.9462</td>\n",
              "      <td>223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>57.6441</td>\n",
              "      <td>62.5</td>\n",
              "      <td>59.9739</td>\n",
              "      <td>230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>62.8521</td>\n",
              "      <td>42.5507</td>\n",
              "      <td>50.7463</td>\n",
              "      <td>357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>11</td>\n",
              "      <td>83.1683</td>\n",
              "      <td>97.6744</td>\n",
              "      <td>89.8396</td>\n",
              "      <td>84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>12</td>\n",
              "      <td>96.8059</td>\n",
              "      <td>98.377</td>\n",
              "      <td>97.5851</td>\n",
              "      <td>788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>13</td>\n",
              "      <td>89.3014</td>\n",
              "      <td>85.0028</td>\n",
              "      <td>87.0991</td>\n",
              "      <td>3055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>14</td>\n",
              "      <td>93.8448</td>\n",
              "      <td>94.7167</td>\n",
              "      <td>94.2787</td>\n",
              "      <td>5199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>15</td>\n",
              "      <td>80.9237</td>\n",
              "      <td>71.836</td>\n",
              "      <td>76.1095</td>\n",
              "      <td>806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>16</td>\n",
              "      <td>88.334</td>\n",
              "      <td>87.6353</td>\n",
              "      <td>87.9833</td>\n",
              "      <td>2105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>17</td>\n",
              "      <td>96.9479</td>\n",
              "      <td>92.4658</td>\n",
              "      <td>94.6538</td>\n",
              "      <td>540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>18</td>\n",
              "      <td>97.3684</td>\n",
              "      <td>99.3074</td>\n",
              "      <td>98.3283</td>\n",
              "      <td>1147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>19</td>\n",
              "      <td>97.6471</td>\n",
              "      <td>96.2319</td>\n",
              "      <td>96.9343</td>\n",
              "      <td>332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>20</td>\n",
              "      <td>98.6207</td>\n",
              "      <td>96.9492</td>\n",
              "      <td>97.7778</td>\n",
              "      <td>286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>21</td>\n",
              "      <td>94.7761</td>\n",
              "      <td>92.2112</td>\n",
              "      <td>93.4761</td>\n",
              "      <td>1397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>22</td>\n",
              "      <td>84.9196</td>\n",
              "      <td>79.0393</td>\n",
              "      <td>81.874</td>\n",
              "      <td>1267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>23</td>\n",
              "      <td>89.9866</td>\n",
              "      <td>94.8972</td>\n",
              "      <td>92.3767</td>\n",
              "      <td>1339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>24</td>\n",
              "      <td>91.2218</td>\n",
              "      <td>84.0437</td>\n",
              "      <td>87.4858</td>\n",
              "      <td>769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>25</td>\n",
              "      <td>92.3567</td>\n",
              "      <td>70.2179</td>\n",
              "      <td>79.7799</td>\n",
              "      <td>290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>26</td>\n",
              "      <td>96.2963</td>\n",
              "      <td>88.6364</td>\n",
              "      <td>92.3077</td>\n",
              "      <td>156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>27</td>\n",
              "      <td>92.9825</td>\n",
              "      <td>80.916</td>\n",
              "      <td>86.5306</td>\n",
              "      <td>106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>29</td>\n",
              "      <td>94.4615</td>\n",
              "      <td>97.1519</td>\n",
              "      <td>95.7878</td>\n",
              "      <td>307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>30</td>\n",
              "      <td>70.7965</td>\n",
              "      <td>78.4314</td>\n",
              "      <td>74.4186</td>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>31</td>\n",
              "      <td>68.4211</td>\n",
              "      <td>75.7282</td>\n",
              "      <td>71.8894</td>\n",
              "      <td>78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>32</td>\n",
              "      <td>65.4762</td>\n",
              "      <td>61.7978</td>\n",
              "      <td>63.5838</td>\n",
              "      <td>110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>33</td>\n",
              "      <td>87.5</td>\n",
              "      <td>51.4706</td>\n",
              "      <td>64.8148</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>34</td>\n",
              "      <td>86.8243</td>\n",
              "      <td>91.4591</td>\n",
              "      <td>89.0815</td>\n",
              "      <td>514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>35</td>\n",
              "      <td>71.4286</td>\n",
              "      <td>55.5556</td>\n",
              "      <td>62.5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>36</td>\n",
              "      <td>100</td>\n",
              "      <td>81.25</td>\n",
              "      <td>89.6552</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>37</td>\n",
              "      <td>88.6957</td>\n",
              "      <td>100</td>\n",
              "      <td>94.0092</td>\n",
              "      <td>102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>38</td>\n",
              "      <td>66.6667</td>\n",
              "      <td>51.2821</td>\n",
              "      <td>57.971</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>39</td>\n",
              "      <td>75.6579</td>\n",
              "      <td>82.1429</td>\n",
              "      <td>78.7671</td>\n",
              "      <td>115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>40</td>\n",
              "      <td>100</td>\n",
              "      <td>100</td>\n",
              "      <td>100</td>\n",
              "      <td>280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>41</td>\n",
              "      <td>71.4286</td>\n",
              "      <td>75</td>\n",
              "      <td>73.1707</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>42</td>\n",
              "      <td>100</td>\n",
              "      <td>76.4706</td>\n",
              "      <td>86.6667</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>43</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>45</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>46</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>accuracy=93.25</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               tag precision   recall  f_score correct_count\n",
              "0                1   99.8092  99.3758  99.5921          3662\n",
              "1                2   94.8054  94.4835  94.6442          7793\n",
              "2                3   91.0111  96.5184  93.6839         16301\n",
              "3                4   99.9766  99.3654  99.6701         12840\n",
              "4                5   91.6667  98.5075   94.964            66\n",
              "5                6   99.7817  87.5479  93.2653           457\n",
              "6                7   97.6374  97.4026  97.5199          2025\n",
              "7                8   67.7812  53.7349  59.9462           223\n",
              "8                9   57.6441     62.5  59.9739           230\n",
              "9               10   62.8521  42.5507  50.7463           357\n",
              "10              11   83.1683  97.6744  89.8396            84\n",
              "11              12   96.8059   98.377  97.5851           788\n",
              "12              13   89.3014  85.0028  87.0991          3055\n",
              "13              14   93.8448  94.7167  94.2787          5199\n",
              "14              15   80.9237   71.836  76.1095           806\n",
              "15              16    88.334  87.6353  87.9833          2105\n",
              "16              17   96.9479  92.4658  94.6538           540\n",
              "17              18   97.3684  99.3074  98.3283          1147\n",
              "18              19   97.6471  96.2319  96.9343           332\n",
              "19              20   98.6207  96.9492  97.7778           286\n",
              "20              21   94.7761  92.2112  93.4761          1397\n",
              "21              22   84.9196  79.0393   81.874          1267\n",
              "22              23   89.9866  94.8972  92.3767          1339\n",
              "23              24   91.2218  84.0437  87.4858           769\n",
              "24              25   92.3567  70.2179  79.7799           290\n",
              "25              26   96.2963  88.6364  92.3077           156\n",
              "26              27   92.9825   80.916  86.5306           106\n",
              "27              29   94.4615  97.1519  95.7878           307\n",
              "28              30   70.7965  78.4314  74.4186            80\n",
              "29              31   68.4211  75.7282  71.8894            78\n",
              "30              32   65.4762  61.7978  63.5838           110\n",
              "31              33      87.5  51.4706  64.8148            35\n",
              "32              34   86.8243  91.4591  89.0815           514\n",
              "33              35   71.4286  55.5556     62.5             5\n",
              "34              36       100    81.25  89.6552            13\n",
              "35              37   88.6957      100  94.0092           102\n",
              "36              38   66.6667  51.2821   57.971            20\n",
              "37              39   75.6579  82.1429  78.7671           115\n",
              "38              40       100      100      100           280\n",
              "39              41   71.4286       75  73.1707            15\n",
              "40              42       100  76.4706  86.6667            13\n",
              "41              43         0        0        -             0\n",
              "42              45         -        0        -             0\n",
              "43              46         -        0        -             0\n",
              "44  accuracy=93.25                                          "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 6.12 s, sys: 95.2 ms, total: 6.21 s\n",
            "Wall time: 3.78 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtpnyowrhHQk",
        "colab_type": "text"
      },
      "source": [
        "## 4.1.2 Neural POS Tagger - Fix Weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMsx-I73hHQl",
        "colab_type": "text"
      },
      "source": [
        "### #TODO 1\n",
        "We would like you create a neural postagger model with keras with the pretrained word embedding as an input. The word embedding should be fixed across training time. To finish this excercise you must train the model and show the evaluation report with this model as shown in the example.\n",
        "\n",
        "(You may want to read about Keras's Masking layer and Trainable parameter)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALy5vNE8hHQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAD-eyD0hHQq",
        "colab_type": "text"
      },
      "source": [
        "## 4.1.3 Neural POS Tagger - Trainable pretrained weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9OE3pw-hHQr",
        "colab_type": "text"
      },
      "source": [
        "### #TODO 2\n",
        "We would like you create a neural postagger model with keras with the pretrained word embedding as an input. However The word embedding is trainable (not fixed). To finish this excercise you must train the model and show the evaluation report with this model as shown in the example.\n",
        "\n",
        "Please note that the given pretrained word embedding only have weights for the vocabuary in BEST corpus.\n",
        "\n",
        "Optionally, you can use your own pretrained word embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OR0aR7VbhHQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CH3d58thHQv",
        "colab_type": "text"
      },
      "source": [
        "### #TODO 3\n",
        "Compare the result between all neural tagger models in 4.1.x and provide a convincing reason and example for the result of these models (which model perform better, why?)\n",
        "\n",
        "(If you use your own weight please state so in the answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJKtOsoRhHQv",
        "colab_type": "text"
      },
      "source": [
        "<b>Write your answer here :</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0NKka14hHQw",
        "colab_type": "text"
      },
      "source": [
        "## 4.2.1 CRF Viterbi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijd1rwTghHQx",
        "colab_type": "text"
      },
      "source": [
        "Your next two tasks are to incorporate Conditional random fields (CRF) to your model. <b>You do not need to use pretrained weight</b>.\n",
        "\n",
        "Keras already implement a CRF neural model for you. However, you need to use the official extension repository for Keras library, call keras-contrib. You should read about keras-contrib crf layer before attempt this exercise section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuybajePhHQy",
        "colab_type": "text"
      },
      "source": [
        "### #TODO 4\n",
        "Use Keras-contrib CRF layer in your model. You should set the layer parameter so it can give the best performance on testing using <b>viterbi algorithm</b>. Your model must use crf for loss function and metric. CRF is quite complex compare to previous example model, so you should train it with more epoch, so it can converge.\n",
        "\n",
        "To finish this excercise you must train the model and show the evaluation report with this model as shown in the example.\n",
        "\n",
        "Do not forget to save this model weight."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEV0q1vAhHQy",
        "colab_type": "code",
        "outputId": "f6e221b8-ec11-40f5-98c0-2e126ffb8c88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 102, 32)           480608    \n",
            "_________________________________________________________________\n",
            "crf_2 (CRF)                  (None, 102, 48)           3984      \n",
            "=================================================================\n",
            "Total params: 484,592\n",
            "Trainable params: 484,592\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miTqOn1QhHQ1",
        "colab_type": "text"
      },
      "source": [
        "## 4.2.2 CRF Marginal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc8r9t5OhHQ2",
        "colab_type": "text"
      },
      "source": [
        "### #TODO 5\n",
        "\n",
        "Use Keras-contrib CRF layer in your model. You should set the layer parameter so it can give the best performance on testing using <b>marginal problabilities</b>. You <b>must not train a new model</b>  but use the pretrained weight from #TODO 4.\n",
        "\n",
        "To finish this excercise you must use the weights from the model trained in previous step and show the evaluation report of marginal problability decoding (testing mode)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMNWCPNmhHQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH-pmLwchHQ5",
        "colab_type": "text"
      },
      "source": [
        "### #TODO 6\n",
        "\n",
        "Please pick the best example that can show the different between CRF that use viterbi and CRF that use marginal problabilities. Compare the result and provide a convincing reason. (Which model perform better, why? / Which model should be faster? Is it true in this case, why?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97x3wy31hHQ6",
        "colab_type": "text"
      },
      "source": [
        "<b>Write your answer here :</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7u-kqwfPhHQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}